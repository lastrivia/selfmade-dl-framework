{%- from "procedure.h.j2" import render_procedure -%}
{%- from "style.j2" import camel -%}
class GradNode{{ camel(internal_name) }}{{ camel(dtype) }} : public GradNode {
public:
    GradNode{{ camel(internal_name) }}{{ camel(dtype) }}({{ init_args }}) :
        {{ init_proc }} {}

    ~GradNode{{ camel(internal_name) }}{{ camel(dtype) }}() override = default;

    std::vector<TensorImpl *> inputs() override {
        std::vector<TensorImpl *> ret;
        ret.reserve({{ tensors|length }});
{%- for i in tensors %}
        if ({{ i }}_->requires_grad_)
            ret.push_back({{ i }}_.object_);
{%- endfor %}
        return ret;
    }

    void backward() override {
        if (
        {%- for i in tensors -%}
            {{ i }}_ver_ != {{ i }}_->version_
            {%- if not loop.last %} || {% endif -%}
        {%- endfor -%}
        )
            throw FatalExcept("autograd: operands modified before backward", __FILE__, __LINE__);

{%- if shape == "broadcast" %}
        bool {% for arg in broadcast_args -%}
            *{{ arg }}_mask = {{ arg }}_mask_workspace_
            {%- if not loop.last %}, {% endif -%}
        {%- endfor -%}
        ;
{% elif shape == "reduction" %}
        bool *mask = mask_workspace_;
{% elif shape == "pooling" %}
        bool *mask = mask_workspace_;
{% endif %}
        DeviceDesc device = tensor_->device();
{%- for i in tensors %}

        // [codegen] backward: {{ i }}
        if ({{ i }}_->requires_grad_) {
{%- for j in procedure[i] %}
{%- set indent = "    " * (j.indent + 3) %}
{{- render_procedure(j, dtype, indent, {"grad_target": i}) }}
{%- endfor %}
        }
{%- endfor %}
    }

private:
{%- for i in members %}
    {{ i }};
{%- endfor %}
};